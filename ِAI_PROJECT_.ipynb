{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FadyRafat0/Car-Price-Prediction/blob/main/%D9%90AI_PROJECT_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsHhNPVXDIh1"
      },
      "source": [
        "# **Car Price Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHo47isseCRK"
      },
      "source": [
        "# Column description\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzwhpOsbDNFz"
      },
      "source": [
        "**Car Id** : represent the id for each car where each car has unique id\n",
        "\n",
        "**Levy**: A levy is a legal seizure of your property to satisfy a tax debt\n",
        "\n",
        "**Manufacturer**: The brand that manfactured the car\n",
        "\n",
        "**Model**: The model of the car\n",
        "\n",
        "**Prod_year** : The year the car producted in\n",
        "\n",
        "**Category**: Category of the car As ( jeep , car , hatchback , etc..)\n",
        "\n",
        "**Leather interior** : is the car interior as the chairs made of leather or not\n",
        "\n",
        "**Feul type** : The type of the feul that the car takes\n",
        "\n",
        "**Engine Volume** : The volume the car takes of feul\n",
        "\n",
        "**Mileage** : The moved Mileage by the car\n",
        "\n",
        "**Cylinders** : how many cylinders the car contains where by increasng the car becomes much better\n",
        "\n",
        "**Gear box type** : Descripe whether the car is automatic or manual\n",
        "\n",
        "**Drive wheels** : Determine whether the car moves by the front tires or the backward tires or by both as jeep cars\n",
        "\n",
        "**Doors** : Determine the number of the doors in the car\n",
        "\n",
        "**Color** : Determine the color of the car\n",
        "\n",
        "**Airbags** : The number of Air bags the car contains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah2fceNA5Qz_"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YXR3i5xcMMT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset,Dataset\n",
        "\n",
        "# feature selection\n",
        "from sklearn.feature_selection import RFE, RFECV\n",
        "import pickle\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.ensemble import RandomForestRegressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UbSmR8rgqfK"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lEffFi-duvz",
        "outputId": "82e83c0d-21b2-4dde-8b1e-c12ba1a83b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2161274410.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_data.csv'"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv('train_data.csv')\n",
        "test_df = pd.read_csv('test_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZe6MEtIPErz"
      },
      "source": [
        "#EDA and Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcLyfUCIeUvr"
      },
      "source": [
        "## First Look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muNxpkNEK2vH"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuSG049OLRDu"
      },
      "outputs": [],
      "source": [
        "test_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvG1XfwZefQL"
      },
      "source": [
        "## Understanding the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXMoYH8md5Qz"
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDv-xzI0LUFG"
      },
      "outputs": [],
      "source": [
        "test_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Boyw0nkhd6pc"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Invt5t6RLYe6"
      },
      "outputs": [],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB1CEEXed71d"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqh7E_JRLfv4"
      },
      "outputs": [],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNiiW0Z1mZhb"
      },
      "source": [
        "###Visualize Categorical Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwnw0ZCb4JzJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='Manufacturer', data=train_df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Distribution of Car Manufacturers')\n",
        "plt.xlabel('Manufacturer')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0puZY5_l63MU"
      },
      "source": [
        "#### From this Graph We conclude that Toyota Cars Has the Most Manfuctured Cars in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4oHq3A9963o"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30, 30))\n",
        "train_df['Category'].value_counts().plot.pie(autopct='%1.1f%%')\n",
        "plt.title('Distribution of Car Categories')\n",
        "plt.ylabel('')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvmBCEq4-xJR"
      },
      "source": [
        "#### From this pie chart we conclude that Sedan is the most Percentage category in our data followed by Jeep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9RHEdqb-5hK"
      },
      "outputs": [],
      "source": [
        "leather_counts = train_df['Leather interior'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(leather_counts, labels=leather_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribution of Leather Interior')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sRgEeAP_exR"
      },
      "source": [
        "#### from the pie chart we conclude that 72% of the cars have Leather interior and this to high compared to the cars that dont have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymN-LkOZ_mps"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Fuel type', data=train_df)\n",
        "plt.title('Distribution of Fuel Types')\n",
        "plt.xlabel('Fuel Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGxp3qc6_xQO"
      },
      "source": [
        "#### From this barchart we conclude that Petrol is the most used Feul type in our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEDsBjeW_60I"
      },
      "outputs": [],
      "source": [
        "gearbox_counts = train_df['Gear box type'].value_counts()\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(gearbox_counts, labels=gearbox_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribution of Gear Box Types')\n",
        "plt.axis('equal')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4BgDl9iApF6"
      },
      "source": [
        "#### From this pie chart we conclude that 70% of the cars in our data is Automatic cars followed by tiptroinic cars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7kvvFhqAxn_"
      },
      "outputs": [],
      "source": [
        "drive_wheels_counts = train_df['Drive wheels'].value_counts()\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(drive_wheels_counts, labels=drive_wheels_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribution of Drive Wheels')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2TxYdAvQkOV"
      },
      "source": [
        "###Visualize Numerical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwHEOyxR4Kdv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['Levy'], kde=True)\n",
        "plt.title('Distribution of Levy')\n",
        "plt.xlabel('Levy')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcimj6gd6hz4"
      },
      "source": [
        "#### The distribution of the Levy values is heavily skewed to the right, with a significant number of vehicles having either very low or zero levy. This suggests that most cars in the dataset come with minimal or no extra cost/tax. However, there are a few cars with very high levy values, which appear to be outliers. These outliers might represent premium or luxury vehicles, or they could indicate data entry errors that warrant further inspection.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9sOEOaQ8E1b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30, 10))\n",
        "sns.histplot(train_df['Prod. year'], kde=True)\n",
        "plt.title('Distribution of Production Year')\n",
        "plt.xlabel('Production Year')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psimik3p9tbl"
      },
      "source": [
        "#### From This BarGraph We conclude That The most Year With Cars Manfuctrued is 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3-mVA9_91Mi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(40, 30))\n",
        "sns.histplot(train_df['Engine volume'], kde=True)\n",
        "plt.title('Distribution of Engine Volume')\n",
        "plt.xlabel('Engine Volume')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-DAqmfwAWQC"
      },
      "source": [
        "#### The distribution of engine volume is right-skewed, with the majority of vehicles having an engine volume between 1.5L and 3.0L, which is typical for standard consumer cars. The peak appears around 2.0L, indicating that it's the most common engine size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zjKih6rAZmM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQYW8Q-afdzB"
      },
      "source": [
        "# PreProcessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGl09M1JhHrD"
      },
      "source": [
        "## Function that combine Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HW8HFxChLvJ"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Preprocesses the input DataFrame by cleaning and validating each column.\n",
        "    Prints unique values, invalid entries, and counts for each column.\n",
        "    Returns the cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'ID' ===\n",
        "    print(\"Unique values for 'ID':\")\n",
        "    print(df['ID'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Price' ===\n",
        "    print(\"Unique values for 'Price':\")\n",
        "    print(df['Price'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Levy' ===\n",
        "    print(\"Unique values for 'Levy':\")\n",
        "    print(df['Levy'].unique())\n",
        "    Nonvalue_levy = df[df['Levy'] == '-']\n",
        "    print(\"'Levy' cells that have no value:\", Nonvalue_levy['Levy'].unique())\n",
        "    print(f\"Number of non-value cells in 'Levy': {Nonvalue_levy.shape[0]}\")\n",
        "\n",
        "    # Replace cells that have '-' with Nulls\n",
        "    df['Levy'] = df['Levy'].replace('-', np.nan)\n",
        "    df['Levy'] = df['Levy'].astype(float)\n",
        "\n",
        "    print(\"Unique values for 'Levy' after cleaning:\")\n",
        "    print(df['Levy'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Manufacturer' ===\n",
        "    print(\"Unique values for 'Manufacturer':\")\n",
        "    print(df['Manufacturer'].unique())\n",
        "    invalid_manufacturers = df[~df['Manufacturer'].astype(str).apply(lambda x: x.isascii())]\n",
        "    print(\"Invalid 'Manufacturer' values:\", invalid_manufacturers['Manufacturer'].unique())\n",
        "    print(f\"Number of invalid 'Manufacturer' values: {invalid_manufacturers.shape[0]}\")\n",
        "\n",
        "    # Deleting rows with invalid manufacturers\n",
        "    df = df[df['Manufacturer'].apply(lambda x: x.isascii() if pd.notnull(x) else True)]\n",
        "\n",
        "    print(\"Unique values for 'Manufacturer' after cleaning:\")\n",
        "    print(df['Manufacturer'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Model' ===\n",
        "    print(\"Unique values for 'Model':\")\n",
        "    print(df['Model'].unique())\n",
        "    invalid_models = df[~df['Model'].astype(str).apply(lambda x: x.isascii()) | df['Model'].astype(str).str.isdigit()]\n",
        "    print(\"Invalid 'Model' values:\", invalid_models['Model'].unique())\n",
        "    print(f\"Number of invalid 'Model' values: {invalid_models.shape[0]}\")\n",
        "\n",
        "    # Remove rows with invalid models\n",
        "    df = df[df['Model'].apply(lambda x: x.isascii() and not str(x).isdigit() if pd.notnull(x) else True)]\n",
        "\n",
        "    print(\"Unique values for 'Model' after cleaning:\")\n",
        "    print(df['Model'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Production Year' ===\n",
        "    print(\"Unique values for 'Prod. year':\")\n",
        "    print(df['Prod. year'].unique())\n",
        "    invalid_prod_year = df['Prod. year'].apply(lambda x: pd.notnull(x) and not str(x).isdigit())\n",
        "    print(\"Non-numeric 'Prod. year' entries:\")\n",
        "    print(df.loc[invalid_prod_year, 'Prod. year'].unique())\n",
        "    print(\"Count of non-numeric 'Prod. year' entries:\", invalid_prod_year.sum())\n",
        "\n",
        "    # Removing rows with invalid production year\n",
        "    df = df[~invalid_prod_year]\n",
        "    df['Prod. year'] = df['Prod. year'].astype(float)\n",
        "\n",
        "    print(\"Unique values for 'Prod. year' after cleaning:\")\n",
        "    print(df['Prod. year'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Category' ===\n",
        "    print(\"Unique values for 'Category':\")\n",
        "    print(df['Category'].unique())\n",
        "    count = df['Category'].isin(['Yes', 'No']).sum()\n",
        "    print(\"Number of rows with 'Yes' or 'No' in 'Category':\", count)\n",
        "\n",
        "    # Remove rows with invalid category entries\n",
        "    df = df[~df['Category'].isin(['Yes', 'No'])]\n",
        "\n",
        "    print(\"Unique values for 'Category' after cleaning:\")\n",
        "    print(df['Category'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Leather Interior' ===\n",
        "    print(\"Unique values for 'Leather interior':\")\n",
        "    print(df['Leather interior'].unique())\n",
        "    count = df['Leather interior'].isin(['Yes', 'No']).sum()\n",
        "    print(\"Number of rows with 'Yes' or 'No' in 'Leather interior':\", count)\n",
        "\n",
        "    # Remove rows with entries other than 'Yes' or 'No'\n",
        "    df = df[df['Leather interior'].isin(['Yes', 'No']) | df['Leather interior'].isna()]\n",
        "\n",
        "    print(\"Unique values for 'Leather interior' after cleaning:\")\n",
        "    print(df['Leather interior'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Fuel Type' ===\n",
        "    print(\"Unique values for 'Fuel type':\")\n",
        "    print(df['Fuel type'].unique())\n",
        "    invalid_fuel_types = df[~df['Fuel type'].astype(str).apply(lambda x: x.isalpha() or x == 'Plug-in Hybrid')]\n",
        "    print(\"Invalid 'Fuel type' values:\", invalid_fuel_types['Fuel type'].unique())\n",
        "    print(f\"Number of invalid 'Fuel type' values: {invalid_fuel_types.shape[0]}\")\n",
        "\n",
        "    # Remove invalid fuel type entries\n",
        "    df = df[df['Fuel type'].apply(lambda x: x.isalpha() or x == 'Plug-in Hybrid' if pd.notnull(x) else True)]\n",
        "\n",
        "    print(\"Unique values for 'Fuel type' after cleaning:\")\n",
        "    print(df['Fuel type'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Engine Volume' ===\n",
        "    print(\"Unique values for 'Engine volume':\")\n",
        "    print(df['Engine volume'].unique())\n",
        "    km_mask = df['Engine volume'].apply(lambda x: isinstance(x, str) and 'km' in x.lower())\n",
        "    print(\"Values with 'km' in 'Engine volume':\", df.loc[km_mask, 'Engine volume'].unique())\n",
        "    print(\"Count of values with 'km' in 'Engine volume':\", km_mask.sum())\n",
        "\n",
        "    # Clean 'Turbo' values and remove 'km' entries\n",
        "    df = df[~km_mask | df['Engine volume'].isna()]\n",
        "    df['Engine volume'] = df['Engine volume'].apply(lambda x: x.split()[0] if isinstance(x, str) and 'Turbo' in x else x)\n",
        "    df['Engine volume'] = df['Engine volume'].astype(float)\n",
        "\n",
        "    print(\"Unique values for 'Engine volume' after cleaning:\")\n",
        "    print(df['Engine volume'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Mileage' ===\n",
        "    print(\"Unique values for 'Mileage':\")\n",
        "    print(df['Mileage'].unique())\n",
        "    invalid_mileage = df[~df['Mileage'].astype(str).str.strip().str.endswith('km') & df['Mileage'].notna()]\n",
        "    print(\"Invalid 'Mileage' values:\", invalid_mileage['Mileage'].unique())\n",
        "    print(\"Number of invalid 'Mileage' values:\", invalid_mileage.shape[0])\n",
        "\n",
        "    # Keep only valid 'km' entries\n",
        "    valid_km_mask = df['Mileage'].astype(str).str.strip().str.endswith('km')\n",
        "    df = df[valid_km_mask | df['Mileage'].isna()]\n",
        "    df['Mileage'] = df['Mileage'].astype(str)\n",
        "    df['Mileage'] = df['Mileage'].str.extract(r'(\\d+)')\n",
        "    df['Mileage'] = df['Mileage'].astype(float)\n",
        "\n",
        "    print(\"Unique values for 'Mileage' after cleaning:\")\n",
        "    print(df['Mileage'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Cylinders' ===\n",
        "    print(\"Unique values for 'Cylinders':\")\n",
        "    print(df['Cylinders'].unique())\n",
        "    strange_cylinders = df[~df['Cylinders'].astype(str).str.isdigit() & df['Cylinders'].notna()]\n",
        "    print(\"Invalid 'Cylinders' values:\", strange_cylinders['Cylinders'].unique())\n",
        "    print(f\"Number of invalid 'Cylinders' values: {strange_cylinders.shape[0]}\")\n",
        "\n",
        "    # Remove rows with invalid cylinder entries\n",
        "    df = df[df['Cylinders'].astype(str).str.isdigit() | df['Cylinders'].isna()]\n",
        "    df['Cylinders'] = df['Cylinders'].astype(float)\n",
        "\n",
        "    print(\"Unique values for 'Cylinders' after cleaning:\")\n",
        "    print(df['Cylinders'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Gear Box Type' ===\n",
        "    print(\"Unique values for 'Gear box type':\")\n",
        "    print(df['Gear box type'].unique())\n",
        "    valid_gearbox_types = ['Automatic', 'Manual', 'Tiptronic', 'Variator']\n",
        "    invalid_gearbox = df[~df['Gear box type'].isin(valid_gearbox_types) & df['Gear box type'].notnull()]\n",
        "    print(\"Invalid 'Gear box type' values:\", invalid_gearbox['Gear box type'].unique())\n",
        "    print(f\"Number of invalid 'Gear box type' values: {invalid_gearbox.shape[0]}\")\n",
        "\n",
        "    # Remove rows with invalid gearbox entries\n",
        "    df = df[df['Gear box type'].isin(valid_gearbox_types) | df['Gear box type'].isna()]\n",
        "\n",
        "    print(\"Unique values for 'Gear box type' after cleaning:\")\n",
        "    print(df['Gear box type'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Drive Wheels' ===\n",
        "    print(\"Unique values for 'Drive wheels':\")\n",
        "    print(df['Drive wheels'].unique())\n",
        "    valid_drive_wheels = ['Front', 'Rear', '4x4']\n",
        "    invalid_drive_wheels = df[~df['Drive wheels'].isin(valid_drive_wheels) & df['Drive wheels'].notnull()]\n",
        "    print(\"Invalid 'Drive wheels' values:\", invalid_drive_wheels['Drive wheels'].unique())\n",
        "    print(f\"Number of invalid 'Drive wheels' values: {invalid_drive_wheels.shape[0]}\")\n",
        "\n",
        "    # Remove rows with invalid drive wheel entries\n",
        "    df = df[df['Drive wheels'].isin(valid_drive_wheels) | df['Drive wheels'].isna()]\n",
        "\n",
        "    print(\"Unique values for 'Drive wheels' after cleaning:\")\n",
        "    print(df['Drive wheels'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Doors' ===\n",
        "    print(\"Unique values for 'Doors':\")\n",
        "    print(df['Doors'].unique())\n",
        "    # Drop the 'Doors' column since it doesn't have meaningful values\n",
        "    df.drop('Doors', axis=1, inplace=True)\n",
        "    print(\"'Doors' column dropped\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Wheel' ===\n",
        "    print(\"Unique values for 'Wheel':\")\n",
        "    print(df['Wheel'].unique())\n",
        "    valid_wheels = ['Left wheel', 'Right-hand drive']\n",
        "    invalid_wheels = df[~df['Wheel'].isin(valid_wheels) & df['Wheel'].notnull()]\n",
        "    print(\"Invalid 'Wheel' values:\", invalid_wheels['Wheel'].unique())\n",
        "    print(f\"Number of invalid 'Wheel' values: {invalid_wheels.shape[0]}\")\n",
        "\n",
        "    # Remove rows with invalid wheel entries\n",
        "    df = df[df['Wheel'].isin(valid_wheels) | df['Wheel'].isna()]\n",
        "\n",
        "    print(\"Unique values for 'Wheel' after cleaning:\")\n",
        "    print(df['Wheel'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Color' ===\n",
        "    print(\"Unique values for 'Color':\")\n",
        "    print(df['Color'].unique())\n",
        "    invalid_colors = df[df['Color'].astype(str).str.isdigit()]\n",
        "    print(\"Invalid 'Color' values:\", invalid_colors['Color'].unique())\n",
        "    print(f\"Number of invalid 'Color' values: {invalid_colors.shape[0]}\")\n",
        "\n",
        "    # Remove rows with invalid color entries\n",
        "    df = df[~df['Color'].astype(str).str.isdigit() | df['Color'].isna()]\n",
        "\n",
        "    print(\"Unique values for 'Color' after cleaning:\")\n",
        "    print(df['Color'].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    # === 'Airbags' ===\n",
        "    print(\"Unique values for 'Airbags':\")\n",
        "    print(df['Airbags'].unique())\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0QltODFh6Uu"
      },
      "outputs": [],
      "source": [
        "train_cleaned = preprocess_data(train_df)\n",
        "test_cleaned = preprocess_data(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdIb_xENiC28"
      },
      "outputs": [],
      "source": [
        "train_cleaned.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkPHua23iL01"
      },
      "outputs": [],
      "source": [
        "test_cleaned.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNX2L8BV6Xz0"
      },
      "source": [
        "## Outliers handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saGH3hXI5xFt"
      },
      "outputs": [],
      "source": [
        "def cap_outliers_iqr(dataframe, columns):\n",
        "    for col in columns:\n",
        "        if pd.api.types.is_numeric_dtype(dataframe[col]):\n",
        "            Q1 = dataframe[col].quantile(0.25)\n",
        "            Q3 = dataframe[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            dataframe[col] = dataframe[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "    return dataframe\n",
        "\n",
        "train_cleaned = cap_outliers_iqr(train_cleaned, train_cleaned.columns)\n",
        "test_cleaned = cap_outliers_iqr(test_cleaned, test_cleaned.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN8DYNd2FRP4"
      },
      "source": [
        "## Nulls handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2Qk2kD36rYr"
      },
      "outputs": [],
      "source": [
        "def fill_missing_values(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().any():\n",
        "            if df[column].dtype == 'object' or df[column].dtype.name == 'category':\n",
        "                mode = df[column].mode()\n",
        "                if not mode.empty:\n",
        "                    df[column].fillna(mode[0], inplace=True)\n",
        "            else:\n",
        "                median = df[column].median()\n",
        "                df[column].fillna(median, inplace=True)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKvDL994iZAT"
      },
      "outputs": [],
      "source": [
        "train_cleaned = fill_missing_values(train_cleaned)\n",
        "test_cleaned = fill_missing_values(test_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJYq53WQ6yzq"
      },
      "outputs": [],
      "source": [
        "train_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsLxKNYK7OvC"
      },
      "outputs": [],
      "source": [
        "test_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZlmMMctcKSi"
      },
      "source": [
        "###Copy CSV File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3UneXIbcPKb"
      },
      "outputs": [],
      "source": [
        "train_cleaned.to_csv('train_cleaned.csv', index=False)\n",
        "test_cleaned.to_csv('test_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0415el4DJUL"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCdT2t4zAnYy"
      },
      "outputs": [],
      "source": [
        "Le = LabelEncoder()\n",
        "for col in train_cleaned.columns:\n",
        "    if train_cleaned[col].dtype == 'object':\n",
        "        train_cleaned[col] = Le.fit_transform(train_cleaned[col])\n",
        "\n",
        "Le = LabelEncoder()\n",
        "for col in test_cleaned.columns:\n",
        "    if test_cleaned[col].dtype == 'object':\n",
        "        test_cleaned[col] = Le.fit_transform(test_cleaned[col])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSyA6JKtTvA2"
      },
      "source": [
        "###Basic info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iANt-k3-TzGU"
      },
      "outputs": [],
      "source": [
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"Set2\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLEANED DATA EXPLORATORY ANALYSIS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Cleaned Train Dataset Shape:\", train_cleaned.shape)\n",
        "print(\"Cleaned Test Dataset Shape:\", test_cleaned.shape)\n",
        "\n",
        "\n",
        "print(train_cleaned.head())\n",
        "print(test_cleaned.head())\n",
        "\n",
        "\n",
        "print(train_cleaned.dtypes)\n",
        "print(test_cleaned.dtypes)\n",
        "\n",
        "\n",
        "print(train_cleaned.describe().transpose())\n",
        "print(test_cleaned.describe().transpose())\n",
        "\n",
        "\n",
        "print(train_cleaned.isna().sum())\n",
        "print(test_cleaned.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVs2VnH-T54M"
      },
      "source": [
        "###Data Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no1paVGuUBq-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(train_cleaned['Price'], kde=True, bins=50, color='darkblue')\n",
        "plt.title('Distribution of Car Prices (Cleaned Data)', fontsize=15)\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Correlation matrix of cleaned data\n",
        "plt.figure(figsize=(14, 12))\n",
        "correlation_matrix = train_cleaned.corr()\n",
        "mask = np.triu(correlation_matrix)\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            mask=mask, linewidths=0.5)\n",
        "plt.title('Correlation Matrix (Cleaned Data)', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature correlations with Price\n",
        "price_corr = correlation_matrix['Price'].sort_values(ascending=False)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=price_corr.values, y=price_corr.index, palette='viridis')\n",
        "plt.title('Feature Correlations with Price (Cleaned Data)', fontsize=15)\n",
        "plt.xlabel('Correlation Coefficient')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DRh_cEmUWJu"
      },
      "outputs": [],
      "source": [
        "# Production Year Analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(x='Prod. year', y='Price', data=train_cleaned, alpha=0.6, color='darkblue')\n",
        "plt.title('Price vs Production Year (Cleaned Data)', fontsize=15)\n",
        "plt.xlabel('Production Year')\n",
        "plt.ylabel('Price')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Engine volume analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(x='Engine volume', y='Price', data=train_cleaned, alpha=0.6, color='darkgreen')\n",
        "plt.title('Price vs Engine Volume (Cleaned Data)', fontsize=15)\n",
        "plt.xlabel('Engine Volume')\n",
        "plt.ylabel('Price')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mileage analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(x='Mileage', y='Price', data=train_cleaned, alpha=0.6, color='indianred')\n",
        "plt.title('Price vs Mileage (Cleaned Data)', fontsize=15)\n",
        "plt.xlabel('Mileage')\n",
        "plt.ylabel('Price')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze top manufacturers by price\n",
        "plt.figure(figsize=(14, 8))\n",
        "top_manufacturers = train_cleaned.groupby('Manufacturer')['Price'].mean().sort_values(ascending=False).head(10)\n",
        "sns.barplot(x=top_manufacturers.index, y=top_manufacturers.values, palette='Set3')\n",
        "plt.title('Top 10 Manufacturers by Average Price (Cleaned Data)', fontsize=15)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Average Price')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Fuel type analysis\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='Fuel type', y='Price', data=train_cleaned, palette='Set2')\n",
        "plt.title('Price Distribution by Fuel Type (Cleaned Data)', fontsize=15)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gear box type analysis\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='Gear box type', y='Price', data=train_cleaned, palette='Set2')\n",
        "plt.title('Price Distribution by Gear Box Type (Cleaned Data)', fontsize=15)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Leather interior analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Leather interior', y='Price', data=train_cleaned, palette='Set2')\n",
        "plt.title('Price Distribution by Leather Interior (Cleaned Data)', fontsize=15)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Drive wheels analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Drive wheels', y='Price', data=train_cleaned, palette='Set2')\n",
        "plt.title('Price Distribution by Drive Wheels (Cleaned Data)', fontsize=15)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cylinders analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Cylinders', y='Price', data=train_cleaned, palette='Set2')\n",
        "plt.title('Price Distribution by Cylinders (Cleaned Data)', fontsize=15)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create pairplot for main numerical features\n",
        "numerical_cols = ['Price', 'Prod. year', 'Engine volume', 'Mileage', 'Cylinders']\n",
        "plt.figure(figsize=(15, 15))\n",
        "sample_df = train_cleaned[numerical_cols].sample(min(1000, len(train_cleaned)))\n",
        "sns.pairplot(sample_df, diag_kind='kde', plot_kws={'alpha': 0.6})\n",
        "plt.suptitle('Pairplot of Main Numerical Features (Cleaned Data)', y=1.02, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUAHchTbVJan"
      },
      "outputs": [],
      "source": [
        "# Distribution of top categorical features after encoding\n",
        "categorical_features = ['Manufacturer', 'Model', 'Category', 'Fuel type', 'Gear box type', 'Drive wheels']\n",
        "plt.figure(figsize=(16, 12))\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    plt.subplot(3, 2, i+1)\n",
        "    sns.histplot(train_cleaned[feature], kde=True, color='darkblue')\n",
        "    plt.title(f'Distribution of {feature} After Encoding', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwshAygzsjA9"
      },
      "source": [
        "## Assign features and target variable to be used in Modeling\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sqjEKGeDkVD"
      },
      "outputs": [],
      "source": [
        "x_train = train_cleaned.drop('Price', axis=1)\n",
        "y_train = train_cleaned['Price']\n",
        "\n",
        "x_test = test_cleaned.drop('Price', axis=1)\n",
        "y_test = test_cleaned['Price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20KGq5MRf1Z3"
      },
      "source": [
        "\n",
        "# Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9lz2M7C0xbD"
      },
      "source": [
        "## **Linear Regression**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nZaGNaD5F65"
      },
      "source": [
        "### Take instances of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l95nwiKA5Gda"
      },
      "outputs": [],
      "source": [
        "X_train_LR = x_train\n",
        "y_train_LR = y_train\n",
        "\n",
        "X_test_LR = x_test\n",
        "y_test_LR = y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEogvUGCdJ-t"
      },
      "source": [
        "### Scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2d-Ju6nN_jf"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_LR_scaled = scaler.fit_transform(X_train_LR)\n",
        "X_test_LR_scaled = scaler.transform(X_test_LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jDkb4n518av"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71xsCFhn2AX6"
      },
      "outputs": [],
      "source": [
        "estimator = LinearRegression()\n",
        "cv = KFold(n_splits=5)\n",
        "selector = RFECV(estimator, step=1, cv=cv, scoring='neg_mean_squared_error')   # Recursive Feature Elimination with Cross-Validation\n",
        "X_train_selected = selector.fit_transform(X_train_LR_scaled, y_train_LR)\n",
        "X_test_selected = selector.transform(X_test_LR_scaled)\n",
        "\n",
        "selected_features = X_train_LR.columns[selector.get_support()]\n",
        "print(\"Selected Features:\", selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZwL2kArN5VM"
      },
      "source": [
        "### Create model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LlYaC1NN5VM"
      },
      "outputs": [],
      "source": [
        "model_LR_scaled = LinearRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmdR1wCMN5VM"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw_PeBd-N5VM"
      },
      "outputs": [],
      "source": [
        "model_LR_scaled.fit(X_train_selected, y_train_LR)             # minimizing the cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMx6esEiN5VN"
      },
      "source": [
        "### Predict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKCXoFtRN5VN"
      },
      "outputs": [],
      "source": [
        "y_pred_LR = model_LR_scaled.predict(X_test_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbKLwgNeN5VN"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uje-atgNN5VN"
      },
      "outputs": [],
      "source": [
        "mse_LR_scaled = mean_squared_error(y_test_LR, y_pred_LR)\n",
        "mae_LR_scaled = mean_absolute_error(y_test_LR, y_pred_LR)\n",
        "r2_LR_scaled = r2_score(y_test_LR, y_pred_LR)\n",
        "\n",
        "print(f\"Mean Square Error of Linear Regression: {mse_LR_scaled:.2f}\")\n",
        "print(f\"Mean Absolute Error of Linear Regression: {mae_LR_scaled:.2f}\")\n",
        "print(f\"R² Score of Linear Regression: {r2_LR_scaled:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0h6feWfN5VN"
      },
      "source": [
        "### Plot Actual vs Predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAHXDayVN5VN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test_LR, y_pred_LR, color='blue', alpha=0.6)\n",
        "plt.plot([y_test_LR.min(), y_test_LR.max()], [y_test_LR.min(), y_test_LR.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Prices using Linear Regression')\n",
        "plt.ylabel('Predicted Prices using Linear Regression')\n",
        "plt.title('Actual vs Predicted Car Prices')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anhqRY9zP9SM"
      },
      "source": [
        "##**Polynomial** **Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxPJbzsacz0k"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbKQNomnSWF9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = train_cleaned.corr(numeric_only=True)\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzuP7lUq9eg8"
      },
      "outputs": [],
      "source": [
        "price_correlations = corr_matrix['Price'].drop('Price').abs().sort_values(ascending=False)\n",
        "print(\"Features most correlated with Price:\")\n",
        "print(price_correlations)\n",
        "\n",
        "\n",
        "#select top features\n",
        "top_features = price_correlations.head(10).index.tolist()\n",
        "print(f\"Selected top features: {top_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoVXZDSOQB9G"
      },
      "outputs": [],
      "source": [
        "X_train_P = x_train[top_features]  # Use selected features\n",
        "y_train_P = y_train\n",
        "X_test_P = x_test[top_features]  # Use selected features\n",
        "y_test_P = y_test\n",
        "\n",
        "\n",
        "\n",
        "scaler_P = StandardScaler()\n",
        "X_train_P_scaled = scaler_P.fit_transform(X_train_P)\n",
        "X_test_P_scaled = scaler_P.transform(X_test_P)\n",
        "\n",
        "\n",
        "#Apply polynomial transformation (after scaling)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_P_scaled)\n",
        "X_test_poly = poly.transform(X_test_P_scaled)\n",
        "\n",
        "\n",
        "print(f\"Original features count: {X_train_P.shape[1]}\")\n",
        "print(f\"Polynomial features count: {X_train_poly.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz9ySToDeBjY"
      },
      "source": [
        "### Create and fit the Linear Regression model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7SXy8K4eLS9"
      },
      "outputs": [],
      "source": [
        "model_poly = LinearRegression()\n",
        "model_poly.fit(X_train_poly, y_train_P)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXI1Ld4_-b0C"
      },
      "source": [
        "###Cross Validation to check overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOXLPdMG-jNU"
      },
      "outputs": [],
      "source": [
        "cv_scores = cross_val_score(model_poly, X_train_poly, y_train_P,\n",
        "                           cv=5, scoring='neg_mean_squared_error')\n",
        "print(f\"Cross-validation MSE scores: {-cv_scores}\")\n",
        "print(f\"Average CV MSE: {-cv_scores.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzieVKSZeOFL"
      },
      "source": [
        "###**Predict**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFI-JQ8AeUPX"
      },
      "outputs": [],
      "source": [
        "y_pred_poly = model_poly.predict(X_test_poly)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wBTJ76ZeXDS"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AY6aypWea75"
      },
      "outputs": [],
      "source": [
        "mse_poly = mean_squared_error(y_test_P, y_pred_poly)\n",
        "r2_poly = r2_score(y_test_P, y_pred_poly)\n",
        "print(f\"Mean Squared Error: {mse_poly}\")\n",
        "print(f\"R² Score: {r2_poly}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "943eemT2-zA2"
      },
      "source": [
        "###Analyze residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR1b0hDp-2-q"
      },
      "outputs": [],
      "source": [
        "residuals = y_test_P - y_pred_poly\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_pred_poly, residuals)\n",
        "plt.axhline(y=0, color='r', linestyle='-')\n",
        "plt.title('Residuals vs Predicted Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.xlabel('Residual Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLNr84CTee9e"
      },
      "source": [
        "### Mileage Attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6B0yMAxe352"
      },
      "outputs": [],
      "source": [
        "if 'Mileage' in top_features:\n",
        "    mileage_idx = top_features.index('Mileage')\n",
        "\n",
        "    X_mean = X_train_P.mean().values.reshape(1, -1)\n",
        "    X_mean_array = np.repeat(X_mean, 100, axis=0)\n",
        "\n",
        "    mileage_range = np.linspace(train_cleaned['Mileage'].min(), train_cleaned['Mileage'].max(), 100)\n",
        "    X_mean_array[:, mileage_idx] = mileage_range\n",
        "\n",
        "    # Scale and transform\n",
        "    X_mean_scaled = scaler_P.transform(X_mean_array)\n",
        "    X_mean_poly = poly.transform(X_mean_scaled)\n",
        "\n",
        "\n",
        "    y_mean_pred = model_poly.predict(X_mean_poly)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(train_cleaned['Mileage'], train_cleaned['Price'], color='blue', alpha=0.3, label='Actual data')\n",
        "    plt.plot(mileage_range, y_mean_pred, color='red', linewidth=2, label='Polynomial fit (degree 2)')\n",
        "    plt.title('Polynomial Regression: Mileage vs Price')\n",
        "    plt.xlabel('Mileage')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJdh13ix08Cy"
      },
      "source": [
        "## **SVR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoonMStL_oIZ"
      },
      "source": [
        "### Split into train/test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srzOJ4Q-1CFo"
      },
      "outputs": [],
      "source": [
        "x_train_SVR = x_train\n",
        "y_train_SVR = y_train\n",
        "\n",
        "x_test_SVR = x_test\n",
        "y_test_SVR = y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXnMaHE4Jngm"
      },
      "source": [
        "### Scaling Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46hBZchCJp7f"
      },
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "scaler_X = StandardScaler()\n",
        "X_train_SVR_scaled = scaler_X.fit_transform(x_train_SVR)\n",
        "X_test_SVR_scaled = scaler_X.transform(x_test_SVR)\n",
        "\n",
        "# Scale the target variable (y)\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_SVR.values.reshape(-1, 1)).ravel()\n",
        "y_test_scaled = scaler_y.transform(y_test_SVR.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "\n",
        "\n",
        "X_train_SVR_selected = X_train_SVR_scaled\n",
        "X_test_SVR_selected = X_test_SVR_scaled\n",
        "print(f\"Using all {X_train_SVR_selected.shape[1]} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLufCzHR_tRl"
      },
      "source": [
        "### Create the Support Vector Regression model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKS9mWoK_wbX"
      },
      "outputs": [],
      "source": [
        "svr_model = SVR()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuprbKn2_x9s"
      },
      "source": [
        "### Tune SVR Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNf4r1K9AsEr"
      },
      "outputs": [],
      "source": [
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist_SVR = {\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'C': np.logspace(-3, 3, 10),\n",
        "    'epsilon': np.logspace(-3, 0, 10),\n",
        "    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 0, 5)),\n",
        "    'degree': [2, 3, 4]  # only used when kernel='poly'\n",
        "}\n",
        "\n",
        "#take sample\n",
        "SAMPLE_SIZE = min(1000, len(X_train_SVR_scaled))\n",
        "indices = np.random.choice(len(X_train_SVR_scaled), SAMPLE_SIZE, replace=False)\n",
        "X_sample = X_train_SVR_scaled[indices]\n",
        "y_sample = y_train_scaled[indices]\n",
        "print(f\"Using {SAMPLE_SIZE} samples for parameter search.\")\n",
        "\n",
        "\n",
        "random_search_SVR = RandomizedSearchCV(\n",
        "    estimator= SVR(),\n",
        "    param_distributions=param_dist_SVR,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_iter=30,\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"Starting parameter search...\")\n",
        "random_search_SVR.fit(X_sample, y_sample)\n",
        "\n",
        "# Get best parameters from RandomizedSearchCV\n",
        "best_svr_model = random_search_SVR.best_estimator_\n",
        "print(f\"Best Parameters: {random_search_SVR.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bg-3TLVBGy9"
      },
      "source": [
        "### Train the best model using the best parameters found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT0Qp3p4BNr1"
      },
      "outputs": [],
      "source": [
        "best_svr_model.fit(X_train_SVR_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPd6MxDdIiKy"
      },
      "source": [
        "###Cross_validation evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE-HWILhIm6N"
      },
      "outputs": [],
      "source": [
        "print(\"Performing cross-validation...\")\n",
        "cv_scores = cross_val_score(\n",
        "    best_svr_model,\n",
        "    X_train_SVR_scaled,\n",
        "    y_train_scaled,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "print(f\"Cross-validation MSE scores: {-cv_scores}\")\n",
        "print(f\"Average CV MSE: {-cv_scores.mean():.4f}, STD: {cv_scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URZGYB0cBNPc"
      },
      "source": [
        "### Make predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt_wIOJ3BTEb"
      },
      "outputs": [],
      "source": [
        "y_pred_scaled = best_svr_model.predict(X_test_SVR_scaled)\n",
        "y_pred_SVR = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1vYKE4RBUf-"
      },
      "source": [
        "### Evaluate the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhJiDVgGBXRv"
      },
      "outputs": [],
      "source": [
        "\n",
        "mse_SVR = mean_squared_error(y_test_SVR, y_pred_SVR)\n",
        "mae_SVR = mean_absolute_error(y_test_SVR, y_pred_SVR)\n",
        "r2_SVR = r2_score(y_test_SVR, y_pred_SVR)\n",
        "\n",
        "print(f\"Mean Squared Error of SVR: {mse_SVR:.2f}\")\n",
        "print(f\"Mean Absolute Error of SVR: {mae_SVR:.2f}\")\n",
        "print(f\"R² Score: {r2_SVR:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IUzZ0SrBZSY"
      },
      "source": [
        "### Plot Actual vs Predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ-tq6VMBcAF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test_SVR, y_pred_SVR, color='blue', alpha=0.6)\n",
        "plt.plot([y_test_SVR.min(), y_test_SVR.max()], [y_test_SVR.min(), y_test_SVR.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Prices using SVR')\n",
        "plt.ylabel('Predicted Prices using SVR')\n",
        "plt.title(f'Actual vs Predicted Car Prices (SVR)\\nBest Params: {random_search_SVR.best_params_}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdHwKifuFW3i"
      },
      "source": [
        "## **XGBoost Algo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QFgs4jCFyyE"
      },
      "outputs": [],
      "source": [
        "xgb = XGBRegressor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAO8oEhGG_2"
      },
      "source": [
        "### Tune the XGB Boost Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kOG9_2xGF2N"
      },
      "outputs": [],
      "source": [
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "rand_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_grid_xgb,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    verbose=3,\n",
        "    return_train_score=True,\n",
        "    n_iter=50,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7uOefzCGXFm"
      },
      "source": [
        "### Dividing data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6MjKuBXGWSW"
      },
      "outputs": [],
      "source": [
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "x_train_Xgb_poly = poly.fit_transform(x_train)\n",
        "x_test_Xgb_poly = poly.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRdMAKJtGfwG"
      },
      "source": [
        "### Scale the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVFIVofgGhwf"
      },
      "outputs": [],
      "source": [
        "scaler_X = StandardScaler()\n",
        "X_train_Xgb_scaled = scaler_X.fit_transform(x_train_Xgb_poly)\n",
        "X_test_Xgb_scaled = scaler_X.transform(x_test_Xgb_poly)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAfFpL8EHls-"
      },
      "source": [
        "### Fit the xgb model on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoXC9SVsGNra"
      },
      "outputs": [],
      "source": [
        "rand_xgb.fit(X_train_Xgb_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwH_41YMwNR"
      },
      "source": [
        "### Visualize the best Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB9WikbGM0eG"
      },
      "outputs": [],
      "source": [
        "print(\"Best Parameters:\", rand_xgb.best_params_)\n",
        "print(\"Best Model:\", rand_xgb.best_estimator_)\n",
        "print(\"Best Score (Neg MSE):\", rand_xgb.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbswTFFq04-E"
      },
      "source": [
        "### Use the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPxaKcRj0-ts"
      },
      "outputs": [],
      "source": [
        "rand_xgb.best_estimator_.fit(X_train_Xgb_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brB13rDUB4Ih"
      },
      "source": [
        "### Use best estmaitor to predict our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVZopGmlB95V"
      },
      "outputs": [],
      "source": [
        "y_pred = rand_xgb.best_estimator_.predict(X_test_Xgb_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW1nLbPfOKba"
      },
      "source": [
        "### Evalute the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsBsGHLROMWg"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test_scaled, y_pred)\n",
        "mae = mean_absolute_error(y_test_scaled, y_pred)\n",
        "r2 = r2_score(y_test_scaled, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"R² Score: {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQseTw67O3yx"
      },
      "source": [
        "## **Neural Networks using Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp5N1ibAPG8X"
      },
      "source": [
        "### Create the custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FC0B0dmSYxd"
      },
      "outputs": [],
      "source": [
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.x = torch.tensor(x_data, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y_data, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PLfw8vYQCdC"
      },
      "source": [
        "### Scale the Poly data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQfWbplbQFC2"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_nn_scaled = scaler.fit_transform(x_train)\n",
        "X_test_nn_scaled = scaler.transform(x_test)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GACaoMoaPfyr"
      },
      "source": [
        "### Constract the Dataset and the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpnkCB_iPjat"
      },
      "outputs": [],
      "source": [
        "train_dataset = RegressionDataset(X_train_nn_scaled ,y_train_scaled)\n",
        "test_dataset = RegressionDataset(X_test_nn_scaled, y_test_scaled)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8UPblO8QYrv"
      },
      "source": [
        "### Form the Linear Regression class for our fully connected nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTP5P8nKQk_4"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf-XfSnuQmjY"
      },
      "source": [
        "### call the model and Choose the Criterion and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VztAgGSQqnv"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression(X_train_nn_scaled.shape[1])\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGKf-ze1Q8qE"
      },
      "source": [
        "### Run the Train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz2VIADVQ8BA"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X.float())\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD2maz9jSuKK"
      },
      "source": [
        "### Eval the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm-87CGaSs5N"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        outputs = model(batch_X.float())\n",
        "        test_loss += criterion(outputs, batch_y).item()\n",
        "    print(f'Test Loss: {test_loss / len(test_loader):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nY3tTubS-Fi"
      },
      "source": [
        "### Calc The MSE And the MAE and The R^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhOnLrmsS9Yu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        all_preds.extend(outputs.squeeze().tolist())\n",
        "        all_targets.extend(targets.squeeze().tolist())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_targets = np.array(all_targets)\n",
        "\n",
        "mse = np.mean((all_preds - all_targets) ** 2)\n",
        "mae = mean_absolute_error(all_targets, all_preds)\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "print(f\"\\n Evaluation Results:\")\n",
        "print(f\" MSE:  {mse:.4f}\")\n",
        "print(f\" MAE:  {mae:.4f}\")\n",
        "print(f\" R² Score: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICXR1LIWDb5_"
      },
      "source": [
        "## **Decision** **Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cja_AtC0E0nf"
      },
      "source": [
        "### Random Search with Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-ZhEcmCE5Vr"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'max_depth': [3, 5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9_XlzD6FNe1"
      },
      "source": [
        "### we will visualize the best parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCCS8DLgFuoS"
      },
      "outputs": [],
      "source": [
        "random_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCE0JO0gGDtV"
      },
      "outputs": [],
      "source": [
        "best_tree = random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz-ukpl9GcT5"
      },
      "outputs": [],
      "source": [
        "poly = PolynomialFeatures(degree=2)\n",
        "x_train_poly = poly.fit_transform(x_train)\n",
        "x_test_poly = poly.transform(x_test)\n",
        "y_train_poly = poly.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_poly = poly.transform(y_test.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvsiUYr7GhUi"
      },
      "outputs": [],
      "source": [
        "best_tree.fit(x_train_poly, y_train_poly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHmQe5TDG3_V"
      },
      "source": [
        "### we will evaluate the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvINnofAHDbX"
      },
      "outputs": [],
      "source": [
        "train_pred = best_tree.predict(x_train_poly)\n",
        "test_pred = best_tree.predict(x_test_poly)\n",
        "\n",
        "train_mse = mean_squared_error(y_train_poly, train_pred)\n",
        "test_mse = mean_squared_error(y_test_poly, test_pred)\n",
        "\n",
        "train_r2 = r2_score(y_train_poly, train_pred)\n",
        "test_r2 = r2_score(y_test_poly, test_pred)\n",
        "\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "\n",
        "print(\"Train R^2:\", train_r2)\n",
        "print(\"Test R^2:\", test_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H7fP1CUJslB"
      },
      "source": [
        "## **Random** **Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtVrCJEYJxIC"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4biPA77LKQEU"
      },
      "source": [
        "### define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x5TTZMeKXfC"
      },
      "outputs": [],
      "source": [
        "param_dist_rf = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z4JM2wLKcMk"
      },
      "outputs": [],
      "source": [
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist_rf,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKpvRluyKgku"
      },
      "source": [
        "### we are going to run our grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR1HQxKgKoc7"
      },
      "outputs": [],
      "source": [
        "random_search_rf.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKARp5WkMJdC"
      },
      "outputs": [],
      "source": [
        "random_search_rf.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQcIqnH8MPZO"
      },
      "outputs": [],
      "source": [
        "best_rf_model=random_search_rf.best_estimator_.fit(x_train_poly, y_train_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5VyviJqw4Wp"
      },
      "outputs": [],
      "source": [
        "best_rf_model.predict(x_test_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAXYoYWJNIzt"
      },
      "outputs": [],
      "source": [
        "train_predictions = best_rf_model.predict(x_train_poly)\n",
        "test_predictions = best_rf_model.predict(x_test_poly)\n",
        "\n",
        "train_mse = mean_squared_error(y_train_poly, train_predictions)\n",
        "train_mae = mean_absolute_error(y_train_poly, train_predictions)\n",
        "train_r2 = r2_score(y_train_poly, train_predictions)\n",
        "\n",
        "test_mse = mean_squared_error(y_test_poly, test_predictions)\n",
        "test_mae = mean_absolute_error(y_test_poly, test_predictions)\n",
        "test_r2 = r2_score(y_test_poly, test_predictions)\n",
        "\n",
        "print(\"Training Set:\")\n",
        "print(f\"  MSE: {train_mse:.4f}\")\n",
        "print(f\"  MAE: {train_mae:.4f}\")\n",
        "print(f\"  R²: {train_r2:.4f}\")\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(f\"  MSE: {test_mse:.4f}\")\n",
        "print(f\"  MAE: {test_mae:.4f}\")\n",
        "print(f\"  R²: {test_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380Te5_Mj4ut"
      },
      "source": [
        "## **Stacking** **using** **XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V76YkpXmkAK4"
      },
      "source": [
        "###  Prepare Meta-Features and Targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dav1w3Q693lC"
      },
      "outputs": [],
      "source": [
        "X_meta = np.column_stack([\n",
        "    y_pred_LR,\n",
        "    y_pred_poly,\n",
        "    y_pred_SVR,\n",
        "    y_pred,\n",
        "    all_preds,\n",
        "    test_pred,\n",
        "    test_predictions\n",
        "])\n",
        "\n",
        "y_meta = y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYIkcopW-CjF"
      },
      "source": [
        "### Split meta data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SxjVjRn-H-d"
      },
      "outputs": [],
      "source": [
        "X_train_meta, X_test_meta, y_train_meta, y_test_meta = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gBwbD6e-Zcp"
      },
      "source": [
        " ### Define and tune meta model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeO49vlS-gbc"
      },
      "outputs": [],
      "source": [
        "xgb_meta = XGBRegressor()\n",
        "\n",
        "param_grid_meta = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "rand_meta_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb_meta,\n",
        "    param_distributions=param_grid_meta,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    verbose=3,\n",
        "    return_train_score=True,\n",
        "    n_iter=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rand_meta_xgb.fit(X_train_meta, y_train_meta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZln8Hrd-0Nz"
      },
      "source": [
        "### Evaluate Meta Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRYZQvr3-7NV"
      },
      "outputs": [],
      "source": [
        "y_pred_meta = rand_meta_xgb.best_estimator_.predict(X_test_meta)\n",
        "\n",
        "mse = mean_squared_error(y_test_meta, y_pred_meta)\n",
        "mae = mean_absolute_error(y_test_meta, y_pred_meta)\n",
        "r2 = r2_score(y_test_meta, y_pred_meta)\n",
        "\n",
        "print(\"Best Parameters:\", rand_meta_xgb.best_params_)\n",
        "print(\"Best Score (Neg MSE):\", rand_meta_xgb.best_score_)\n",
        "print(f\"Meta Model - MSE: {mse:.4f}\")\n",
        "print(f\"Meta Model - MAE: {mae:.4f}\")\n",
        "print(f\"Meta Model - R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUBi2LdGlrs_"
      },
      "source": [
        "#GUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLO0zT-2lxJk"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o4kxuXXx6IN"
      },
      "outputs": [],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5xQDSjgx-nU"
      },
      "outputs": [],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pHo47isseCRK",
        "ah2fceNA5Qz_",
        "pcLyfUCIeUvr",
        "IGl09M1JhHrD",
        "SJdh13ix08Cy",
        "CdHwKifuFW3i",
        "SQseTw67O3yx",
        "ICXR1LIWDb5_",
        "Y9_XlzD6FNe1",
        "0H7fP1CUJslB",
        "380Te5_Mj4ut",
        "DUBi2LdGlrs_"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}